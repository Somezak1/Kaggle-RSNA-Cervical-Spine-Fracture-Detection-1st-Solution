<html>
<head>
  <title>Evernote Export</title>
  <basefont face="微软雅黑 Light" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/606060 (zh-CN, DDL); Windows/10.0.0 (Win64); EDAMVersion=V2;"/>
  <style>
    body, td {
      font-family: 微软雅黑 Light;
      font-size: 14pt;
    }
  </style>
</head>
<body>
<a name="20168"/>
<h1>【Qishen Ha】冠军方案</h1>
<div>
<table bgcolor="#D4DDE5" border="0">
<tr><td><b>创建时间：</b></td><td><i>2022/12/12 18:14</i></td></tr>
<tr><td><b>更新时间：</b></td><td><i>2022/12/13 11:34</i></td></tr>
<tr><td><b>作者：</b></td><td><i>不理不理</i></td></tr>
</table>
</div>
<br/>

<div>
<span><div><div style="text-align: left;"><br/></div><div><a href="https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/discussion/362607#2062717" style="font-size: 18pt; font-weight: bold;">1st Place Solution</a></div><div>Thanks to the organizers and congrats to all the winners and those who worked hard to develop new pipelines and stuck with it until the end of the competition.</div><div><br/></div><div>This is a very interesting competition, because we can think of many different ways to approach this dataset. Therefore the most important thing for this competition is to develop a reasonable pipeline, followed by optimization of the model.</div><div><br/></div><div><br/></div><div><span style="font-size: 18pt; color: unset; font-family: unset; font-weight: bold;">Code</span></div><div><a href="https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/discussion/362787">https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/discussion/362787</a></div><div style="text-align: center;"><a href="【Qishen Ha】冠军方案_files/rsna-2022-1st-place-solution-train-stage1.ipynb"><img src="【Qishen Ha】冠军方案_files/73fc4dfda7fc3fc5b125e2ad0e0112db.png" alt="rsna-2022-1st-place-solution-train-stage1.ipynb"></a></div><div style="text-align: center;"><a href="【Qishen Ha】冠军方案_files/rsna-2022-1st-place-solution-train-stage21.ipynb"><img src="【Qishen Ha】冠军方案_files/fd11737bcd6876ae0f737d9e08ae4f91.png" alt="rsna-2022-1st-place-solution-train-stage21.ipynb"></a></div><div style="text-align: center;"><a href="【Qishen Ha】冠军方案_files/rsna-2022-1st-place-solution-train-stage22.ipynb"><img src="【Qishen Ha】冠军方案_files/a00e9df39c2f9fe033f5666818b761ec.png" alt="rsna-2022-1st-place-solution-train-stage22.ipynb"></a></div><div style="text-align: center;"><a href="【Qishen Ha】冠军方案_files/rsna-2022-1st-place-solution-inference.ipynb"><img src="【Qishen Ha】冠军方案_files/872bcc618067fae114d923477f91e8a3.png" alt="rsna-2022-1st-place-solution-inference.ipynb"></a></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: 18pt; color: unset; font-family: unset; font-weight: bold;">Summary</span></div><div>I designed a 2-stage pipeline to deal with this problem.</div><div><br/></div><div>stage1: 3D semantic segmentation -&gt; stage2: 2.5D w/ LSTM classification.</div><div><br/></div><div>In addition, there are 2 different types of classification models in stage2.</div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: 18pt; color: unset; font-family: unset; font-weight: bold;">3D Semantic Segmentation</span></div><div>For 3D semantic segmentation, we only have 87 samples w/ 3d mask in the dataset, but it's sufficient to train 3D semantic segmentation models with good performance.</div><div><br/></div><div>I use 128x128x128 input, to train resnet18d or efficientnet v2s + unet model, for segmenting C[1-7] vertebraes (7ch output).</div><div><br/></div><div>After the training was completed, I predicted 3d masks for each vertebrae for all 2k samples in the training set.</div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: unset; color: unset; font-family: unset;">Here is an example of predicted masks of C[1-7] vertebrae. Center slice of x, y, z dimension view, from left to right.</span></div><div style="text-align: center;"><img src="【Qishen Ha】冠军方案_files/1.png" type="image/png" data-filename="1.png"/></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: 18pt; color: unset; font-family: unset; font-weight: bold;">Prepare Data for Classification</span></div><div>Next step is to prepare data for classification.</div><div><br/></div><div>First using the predicted 3D mask for each vertebrae, we can crop out 7 vertebraes from a single original 3d image (there might be multiple vertebraes shown in a single crop, but It's fine). At this moment, we cropped 2k * 7 = 14k samples and for each sample there is only one single binary label.</div><div><br/></div><div>Then for each vertebrae sample, I extracted 15 slices evenly by z-dimension, and for each slice, I further extracted +-2 adjacent slices to form an image with 5 channels. E.g if a 3D vertebrae sample have a shape of (128, 128, 30), I extracted 0th, 2nd, 4th, 6th….26th, 28th slices, then for example for the 2nd one, I use 0th~4th slices to form a 5-channel image.</div><div><br/></div><div>In addition, I added the predicted mask of corresponding vertebrae as the 6th channel to each image, as a way to exclude the effect of having multiple vertebraes in a single sample.</div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: unset; color: unset; font-family: unset;">Here is an example of one slice of a single vertebrae, and its predicted mask (with augmentations). We can see that the left half of the vertebrae in the image do not belong to the vertebrae specified by this crop.</span></div><div style="text-align: center;"><img src="【Qishen Ha】冠军方案_files/2.png" type="image/png" data-filename="2.png"/></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: 18pt; color: unset; font-family: unset; font-weight: bold;">2.5D + LSTM Classification</span></div><div>We now have 14k 3D training samples of vertebrae. Theoretically the easiest way to deal with this data is to train 3D CNN on it. But unfortunately this method does not work. Training a 3D CNN on this data did not give me satisfactory results.</div><div><br/></div><div>So I backed off and chose the 2.5D approach. Here 2.5D means that each 2D slice in a vertebrae sample has the information of several adjacent slices, so it is written 2.5D. But the model is a normal 2D CNN with 5-channels input.</div><div><br/></div><div>The structure of this model is that, I first input 15 slices from a single sample into a 2D CNN, extracted out features of each slice, and then follow it with an LSTM model. So that the whole model can learn the features of the whole vertebrae. I call it type1 model ↓</div><div style="text-align: center;"><img src="【Qishen Ha】冠军方案_files/3.png" type="image/png" data-filename="3.png"/></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: unset; color: unset; font-family: unset;">This model structure above, while being able to train a single vertebrae for fracture, does not able to train the patient as a whole for the presence of a fracture. So I designed another model.</span></div><div><br/></div><div>The second classification model is basically the same as the one above, except that it treats a patient as one training sample (the model above treats a vertebrae as one training sample). This model is fed with 7x15 2D images at the same time, so that it has the ability to learn patient_overall labels. I call it type2 model ↓</div><div style="text-align: center;"><img src="【Qishen Ha】冠军方案_files/4.png" type="image/png" data-filename="4.png"/></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: unset; color: unset; font-family: unset;">However, the disadvantage of this model is that it takes up too much GPU memory and therefore can only use small backbones (Imagine a model with batch_size = 1 that has to be trained on 105 images at one time, it is insane).</span></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: 18pt; color: unset; font-family: unset; font-weight: bold;">Final Submission</span></div><div><span style="font-size: unset; color: unset; font-family: unset;">3D Seg</span></div><ul><li><div><span style="font-size: unset; color: unset; font-family: unset;">5fold resnet 18d unet (128x128x128)</span></div></li><li><div><span style="font-size: unset; color: unset; font-family: unset;">5fold effv2 s (128x128x128)</span></div></li></ul><div><span style="font-size: unset;"><br/></span></div><div><span style="font-size: unset; color: unset; font-family: unset;">2.5D Cls</span></div><ul><li><div><span style="font-size: unset; color: unset; font-family: unset;">Type1 5fold effv2s (512x512)</span></div></li><li><div><span style="font-size: unset; color: unset; font-family: unset;">Type1 5fold convnext tiny (384x384)</span></div></li><li><div><span style="font-size: unset; color: unset; font-family: unset;">Type2 5fold convnext nano (512x512)</span></div></li><li><div><span style="font-size: unset; color: unset; font-family: unset;">Type2 2fold convnext pico (512x512)</span></div></li><li><div><span style="font-size: unset; color: unset; font-family: unset;">Type2 2fold convnext tiny (384x384)</span></div></li><li><div><span style="font-size: unset; color: unset; font-family: unset;">Type2 2fold nfnet l0 (384x384)</span></div></li></ul><div><br/></div><div>The submission time is 7.5 hours.</div><div><br/></div><div>Thanks to timm library for having so good implementation of those models. I always using it.</div><div><span style="font-size: unset;"><br/></span></div></div><div><br/></div></span>
</div></body></html> 