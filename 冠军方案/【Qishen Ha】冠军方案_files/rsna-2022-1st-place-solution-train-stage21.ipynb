{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1st Place Solution Training 2.5D Classification Type1\n\nHi all,\n\nI'm very exciting to writing this notebook and the summary of our solution here.\n\nThis is small version of training my final models (stage2 type1), using efficientnetv2_s as backbone, and 224x224 as input.\n\nAfter all stage1 models are trained, then we can use those model to predict 3D masks for all training samples (2k)\n\nThen use those predicted masks to crop out all vertebraes (2k * 7 = 14k)\n\nI'll skip the code of predicting 3D maks and cropping vertebraes, but just uploaded the dataset of cropped vertebraes (https://www.kaggle.com/datasets/haqishen/rsna-cropped-2d-224-0920-2m)\n\nNow let's use this dataset to train a 2.5D classification with LSTM (Type1)\n\n**NOTE: The training time is too long for Kaggle kernels so you should run it locally**\n\nTo see more details of my solution: https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/discussion/362607\n\n* Train Stage1 Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage1\n* Train Stage2 (Type1) Notebook: This notebook\n* Train Stage2 (Type2) Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage2-type2\n* Inference Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-inference\n\n\n**If you find these notebooks helpful please upvote. Thanks!**","metadata":{}},{"cell_type":"code","source":"DEBUG = False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-18T09:38:56.774996Z","iopub.execute_input":"2022-11-18T09:38:56.775425Z","iopub.status.idle":"2022-11-18T09:38:56.787157Z","shell.execute_reply.started":"2022-11-18T09:38:56.775378Z","shell.execute_reply":"2022-11-18T09:38:56.785895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport gc\nimport ast\nimport cv2\nimport time\nimport timm\nimport pickle\nimport random\nimport argparse\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom PIL import Image\nfrom tqdm import tqdm\nimport albumentations\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.cuda.amp as amp\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\n%matplotlib inline\nrcParams['figure.figsize'] = 20, 8\ndevice = torch.device('cuda')\ntorch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2022-11-18T09:38:56.789538Z","iopub.execute_input":"2022-11-18T09:38:56.793759Z","iopub.status.idle":"2022-11-18T09:39:00.759974Z","shell.execute_reply.started":"2022-11-18T09:38:56.793720Z","shell.execute_reply":"2022-11-18T09:39:00.758667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"image_size = 224\nn_slice_per_c = 15\ndata_dir = '../input/rsna-cropped-2d-224-0920-2m/cropped_2d_224_15_ext0_5ch_0920_2m/cropped_2d_224_15_ext0_5ch_0920_2m'\nuse_amp = True\nos.makedirs('./logs', exist_ok=True)\nos.makedirs('./models', exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-18T09:39:00.763325Z","iopub.execute_input":"2022-11-18T09:39:00.764386Z","iopub.status.idle":"2022-11-18T09:39:00.773611Z","shell.execute_reply.started":"2022-11-18T09:39:00.764347Z","shell.execute_reply":"2022-11-18T09:39:00.772410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms_train = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.HorizontalFlip(p=0.5),\n    albumentations.VerticalFlip(p=0.5),\n    albumentations.Transpose(p=0.5),  # switch X and Y axis\n    albumentations.RandomBrightness(limit=0.1, p=0.7),\n    albumentations.ShiftScaleRotate(shift_limit=0.3, scale_limit=0.3, rotate_limit=45, border_mode=4, p=0.7),\n    # Randomly apply affine transforms: translate, scale and rotate the input\n    \n    albumentations.OneOf([\n        albumentations.MotionBlur(blur_limit=3),          # Apply motion blur to the input image using a random-sized kernel\n        albumentations.MedianBlur(blur_limit=3),          # Blur the input image using a median filter with a random aperture linear size.\n        albumentations.GaussianBlur(blur_limit=3),        # Blur the input image using a Gaussian filter with a random kernel size\n        albumentations.GaussNoise(var_limit=(3.0, 9.0)),  # Apply gaussian noise to the input image\n    ], p=0.5),\n    \n    # In medical imaging problems, non-rigid transformations help to augment the data. \n    # It is unclear if they will help with this problem, but let's look at them. We will consider ElasticTransform, GridDistortion, OpticalDistortion.\n    # https://albumentations.ai/docs/examples/example_kaggle_salt/#opticaldistortion\n    albumentations.OneOf([\n        albumentations.OpticalDistortion(distort_limit=1.),\n        albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n    ], p=0.5),\n\n    albumentations.Cutout(max_h_size=int(image_size * 0.5), max_w_size=int(image_size * 0.5), num_holes=1, p=0.5),  # CoarseDropout of the square regions in the image\n])\n\ntransforms_valid = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n])","metadata":{"execution":{"iopub.status.busy":"2022-11-18T09:39:00.775551Z","iopub.execute_input":"2022-11-18T09:39:00.776404Z","iopub.status.idle":"2022-11-18T09:39:00.792124Z","shell.execute_reply.started":"2022-11-18T09:39:00.776364Z","shell.execute_reply":"2022-11-18T09:39:00.790492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataFrame","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(os.path.join(f'../input/rsna-cropped-2d-224-0920-2m/train_seg.csv'))\ndf = df.sample(16).reset_index(drop=True) if DEBUG else df\n\n\nsid = []\ncs = []\nlabel = []\nfold = []\nfor _, row in df.iterrows():\n    for i in [1,2,3,4,5,6,7]:\n        sid.append(row.StudyInstanceUID)\n        cs.append(i)\n        label.append(row[f'C{i}'])\n        fold.append(row.fold)\n\ndf = pd.DataFrame({\n    'StudyInstanceUID': sid,\n    'c': cs,\n    'label': label,\n    'fold': fold\n})\n# 2018 x 7 个样本，每个病人的7节颈椎分别作为一个样本，标签是该颈椎是否骨折\ndf.tail()","metadata":{"execution":{"iopub.status.busy":"2022-11-18T09:39:00.794254Z","iopub.execute_input":"2022-11-18T09:39:00.795151Z","iopub.status.idle":"2022-11-18T09:39:01.445697Z","shell.execute_reply.started":"2022-11-18T09:39:00.795115Z","shell.execute_reply":"2022-11-18T09:39:01.444765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class CLSDataset(Dataset):\n    '''\n    从某病人某节颈椎骨的众多CT图像中均匀采样15张图片，每张图片由附近5张切片和1张掩码拼接而成，返回的images维度为(15, 6, 224, 224)\n    '''\n    def __init__(self, df, mode, transform):\n\n        self.df = df.reset_index()\n        self.mode = mode\n        self.transform = transform\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        cid = row.c\n        \n        images = []\n        \n        for ind in list(range(n_slice_per_c)):\n            filepath = os.path.join(data_dir, f'{row.StudyInstanceUID}_{cid}_{ind}.npy')\n            image = np.load(filepath)\n            # image: (224, 224, 6)  0 - 255\n            image = self.transform(image=image)['image']\n            image = image.transpose(2, 0, 1).astype(np.float32) / 255.\n            # image: (6, 224, 224)  0 - 1\n            images.append(image)\n        images = np.stack(images, 0)\n        # images: (15, 6, 224, 224)  0 - 1\n\n        if self.mode != 'test':\n            images = torch.tensor(images).float()\n            labels = torch.tensor([row.label] * n_slice_per_c).float()\n            \n            if self.mode == 'train' and random.random() < 0.2:\n                indices = torch.randperm(images.size(0))\n                images = images[indices]\n                # # images: (15, 6, 224, 224)  0 - 1  打乱images中15张(6, 224, 224)'图片'的顺序\n\n            return images, labels\n            # torch.Size([15, 6, 224, 224]), torch.Size([15])\n        else:\n            return torch.tensor(images).float()","metadata":{"execution":{"iopub.status.busy":"2022-11-18T09:39:01.450100Z","iopub.execute_input":"2022-11-18T09:39:01.452409Z","iopub.status.idle":"2022-11-18T09:39:01.467998Z","shell.execute_reply.started":"2022-11-18T09:39:01.452368Z","shell.execute_reply":"2022-11-18T09:39:01.467114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rcParams['figure.figsize'] = 20,8\n\ndf_show = df\ndataset_show = CLSDataset(df_show, 'train', transform=transforms_train)\nloader_show = torch.utils.data.DataLoader(dataset_show, batch_size=8, shuffle=True, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2022-11-18T09:39:01.473748Z","iopub.execute_input":"2022-11-18T09:39:01.476866Z","iopub.status.idle":"2022-11-18T09:39:01.491923Z","shell.execute_reply.started":"2022-11-18T09:39:01.476827Z","shell.execute_reply":"2022-11-18T09:39:01.490749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, axarr = plt.subplots(2,4)\nfor p in range(4):\n    idx = p * 20\n    imgs, lbl = dataset_show[idx]\n    axarr[0, p].imshow(imgs[7][:3].permute(1, 2, 0))\n    axarr[1, p].imshow(imgs[7][-1])","metadata":{"execution":{"iopub.status.busy":"2022-11-18T09:39:23.180822Z","iopub.execute_input":"2022-11-18T09:39:23.181830Z","iopub.status.idle":"2022-11-18T09:39:26.236371Z","shell.execute_reply.started":"2022-11-18T09:39:23.181776Z","shell.execute_reply":"2022-11-18T09:39:26.235466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class TimmModel(nn.Module):\n    '''\n    将一个病人某颈椎骨的15张图片，分别用encoder编码，每个样本因此会得到15个1280向量\n    将这15个1280向量通过双向双层LSTM提取信息，得到15个512维向量，每个512向量都包含了该病人该颈椎骨从前至后的所有信息\n    因此一个病人某颈椎骨的15张图片，就可以作为15个样本\n    '''\n    def __init__(self, backbone, pretrained=False):\n        super(TimmModel, self).__init__()\n\n        self.encoder = timm.create_model(\n            backbone,\n            in_chans=6,\n            num_classes=1,\n            features_only=False,\n            drop_rate=0.,\n            drop_path_rate=0.,\n            pretrained=pretrained\n        )\n\n        if 'efficient' in backbone:\n            hdim = self.encoder.conv_head.out_channels\n            # hdim: 1200\n            self.encoder.classifier = nn.Identity()\n        elif 'convnext' in backbone:\n            hdim = self.encoder.head.fc.in_features\n            self.encoder.head.fc = nn.Identity()\n\n\n        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=0., bidirectional=True, batch_first=True)\n        self.head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0.3),\n            nn.LeakyReLU(0.1),\n            nn.Linear(256, 1),\n        )\n\n    def forward(self, x):  # (8, 15, 6, 224, 224)\n        bs = x.shape[0]\n        x = x.view(bs * n_slice_per_c, 6, image_size, image_size)         # [120, 6, 224, 224]\n        feat = self.encoder(x)                                            # [120, 1280]\n        feat = feat.view(bs, n_slice_per_c, -1)                           # [8, 15, 1280]\n        feat, _ = self.lstm(feat)                                         # [8, 15, 512]\n        feat = feat.contiguous().view(bs * n_slice_per_c, -1)             # [120, 512]\n        feat = self.head(feat)                                            # [120, 1]\n        feat = feat.view(bs, n_slice_per_c).contiguous()                  # [8, 15]\n\n        return feat\n    \n\n# batch size = 8\n# ====================================================================================================\n# Layer (type:depth-idx)                             Output Shape              Param #\n# ====================================================================================================\n# input shape: [8, 15, 6, 224, 224]\n# TimmModel                                          [8, 15]                   --\n# reshape:  [8, 15, 6, 224, 224]  →  [120, 6, 224, 224]\n# ├─EfficientNet: 1-1                                [120, 1280]                --\n# │    └─Conv2dSame: 2-1                             [120, 24, 112, 112]        1,296\n# │    └─BatchNormAct2d: 2-2                         [120, 24, 112, 112]        48\n# │    │    └─Identity: 3-1                          [120, 24, 112, 112]        --\n# │    │    └─SiLU: 3-2                              [120, 24, 112, 112]        --\n# │    └─Sequential: 2-3                             [120, 256, 7, 7]           --\n# │    │    └─Sequential: 3-3                        [120, 24, 112, 112]        10,464\n# │    │    └─Sequential: 3-4                        [120, 48, 56, 56]          303,552\n# │    │    └─Sequential: 3-5                        [120, 64, 28, 28]          589,184\n# │    │    └─Sequential: 3-6                        [120, 128, 14, 14]         917,680\n# │    │    └─Sequential: 3-7                        [120, 160, 14, 14]         3,463,840\n# │    │    └─Sequential: 3-8                        [120, 256, 7, 7]           14,561,832\n# │    └─Conv2d: 2-4                                 [120, 1280, 7, 7]          327,680\n# │    └─BatchNormAct2d: 2-5                         [120, 1280, 7, 7]          2,560\n# │    │    └─Identity: 3-9                          [120, 1280, 7, 7]          --\n# │    │    └─SiLU: 3-10                             [120, 1280, 7, 7]          --\n# │    └─SelectAdaptivePool2d: 2-6                   [120, 1280]                --\n# │    │    └─AdaptiveAvgPool2d: 3-11                [120, 1280, 1, 1]          --\n# │    │    └─Flatten: 3-12                          [120, 1280]                --\n# │    └─Identity: 2-7                               [120, 1280]                --\n# reshape:  [120, 1280]  →  [8, 15, 1280]   8 x 15 个 [6, 224, 224] 的图像被 encoder 编码成 1280 的向量\n# ├─LSTM: 1-2                                        [8, 15, 512]              4,726,784\n# ├─Sequential: 1-3                                  [120, 1]                   --\n# │    └─Linear: 2-8                                 [120, 256]                 131,328\n# │    └─BatchNorm1d: 2-9                            [120, 256]                 512\n# │    └─Dropout: 2-10                               [120, 256]                 --\n# │    └─LeakyReLU: 2-11                             [120, 256]                 --\n# │    └─Linear: 2-12                                [120, 1]                   257\n# ====================================================================================================\n# Total params: 25,037,017\n# Trainable params: 25,037,017\n# Non-trainable params: 0\n# Total mult-adds (G): 85.85\n# ====================================================================================================\n# Input size (MB): 36.13\n# Forward/backward pass size (MB): 2926.68\n# Params size (MB): 99.53\n# Estimated Total Size (MB): 3062.34\n# ====================================================================================================","metadata":{"execution":{"iopub.status.busy":"2022-11-15T08:04:46.890844Z","iopub.execute_input":"2022-11-15T08:04:46.891193Z","iopub.status.idle":"2022-11-15T08:04:46.903913Z","shell.execute_reply.started":"2022-11-15T08:04:46.891161Z","shell.execute_reply":"2022-11-15T08:04:46.902904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss & Metric","metadata":{}},{"cell_type":"code","source":"bce = nn.BCEWithLogitsLoss(reduction='none')\n\n\ndef criterion(logits, targets, activated=False):\n    if activated:\n        losses = nn.BCELoss(reduction='none')(logits.view(-1), targets.view(-1))\n    else:\n        losses = bce(logits.view(-1), targets.view(-1))\n    losses[targets.view(-1) > 0] *= 2.\n    norm = torch.ones(logits.view(-1).shape[0]).to(device)\n    norm[targets.view(-1) > 0] *= 2\n    return losses.sum() / norm.sum()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:46:32.740646Z","iopub.execute_input":"2022-10-29T08:46:32.741112Z","iopub.status.idle":"2022-10-29T08:46:32.74936Z","shell.execute_reply.started":"2022-10-29T08:46:32.741073Z","shell.execute_reply":"2022-10-29T08:46:32.747958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train & Valid func","metadata":{}},{"cell_type":"code","source":"def mixup(input, truth, clip=[0, 1]):\n    indices = torch.randperm(input.size(0))\n    shuffled_input = input[indices]\n    shuffled_labels = truth[indices]\n\n    lam = np.random.uniform(clip[0], clip[1])\n    input = input * lam + shuffled_input * (1 - lam)\n    return input, truth, shuffled_labels, lam\n\n\ndef train_func(model, loader_train, optimizer, scaler=None):\n    model.train()\n    train_loss = []\n    bar = tqdm(loader_train)\n    for images, targets in bar:\n        optimizer.zero_grad()\n        images = images.cuda()\n        targets = targets.cuda()\n        # images: torch.Size([15, 6, 224, 224])   targets: torch.Size([15])\n        \n        do_mixup = False\n        if random.random() < 0.5:\n            do_mixup = True\n            images, targets, targets_mix, lam = mixup(images, targets)\n\n        with amp.autocast():\n            logits = model(images)\n            loss = criterion(logits, targets)\n            if do_mixup:\n                loss11 = criterion(logits, targets_mix)\n                loss = loss * lam  + loss11 * (1 - lam)\n        train_loss.append(loss.item())\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        bar.set_description(f'smth:{np.mean(train_loss[-30:]):.4f}')\n\n    return np.mean(train_loss)\n\n\ndef valid_func(model, loader_valid):\n    model.eval()\n    valid_loss = []\n    gts = []\n    outputs = []\n    bar = tqdm(loader_valid)\n    with torch.no_grad():\n        for images, targets in bar:\n            images = images.cuda()\n            targets = targets.cuda()\n\n            logits = model(images)\n            loss = criterion(logits, targets)\n            \n            gts.append(targets.cpu())\n            outputs.append(logits.cpu())\n            valid_loss.append(loss.item())\n            \n            bar.set_description(f'smth:{np.mean(valid_loss[-30:]):.4f}')\n\n    outputs = torch.cat(outputs)\n    gts = torch.cat(gts)\n    valid_loss = criterion(outputs, gts).item()\n\n    return valid_loss","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:46:32.751519Z","iopub.execute_input":"2022-10-29T08:46:32.75212Z","iopub.status.idle":"2022-10-29T08:46:32.766761Z","shell.execute_reply.started":"2022-10-29T08:46:32.752079Z","shell.execute_reply":"2022-10-29T08:46:32.765711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 学习率展示\nrcParams['figure.figsize'] = 20, 5\nm = TimmModel('tf_efficientnetv2_s_in21ft1k')\noptimizer = optim.AdamW(m.parameters(), lr=23e-5)\nscheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 75, eta_min=23e-6)\n\nlrs = []\nfor epoch in range(1, 75+1):\n    scheduler_cosine.step(epoch-1)\n    lrs.append(optimizer.param_groups[0][\"lr\"])\nplt.plot(range(len(lrs)), lrs)\nplt.title('Learning Rate (Epochs: 75   Max: 23e-5   Min: 23e-6)', fontsize=20)\nplt.tick_params(labelsize=15)\nplt.grid()\n_ = plt.xticks(np.arange(0, 80, 5))\n_ = plt.yticks([0, 5e-5, 10e-5, 15e-5, 20e-5, 25e-5])","metadata":{"execution":{"iopub.status.busy":"2022-11-15T08:35:06.476677Z","iopub.execute_input":"2022-11-15T08:35:06.477101Z","iopub.status.idle":"2022-11-15T08:35:06.722068Z","shell.execute_reply.started":"2022-11-15T08:35:06.477065Z","shell.execute_reply":"2022-11-15T08:35:06.721132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def run(fold):\n    kernel_type = '0920_1bonev2_effv2s_224_15_6ch_augv2_mixupp5_drl3_rov1p2_bs8_lr23e5_eta23e6_50ep'\n    log_file = os.path.join('./logs', f'{kernel_type}.txt')\n    model_file = os.path.join('./models', f'{kernel_type}_fold{fold}_best.pth')\n\n    train_ = df[df['fold'] != fold].reset_index(drop=True)\n    valid_ = df[df['fold'] == fold].reset_index(drop=True)\n    dataset_train = CLSDataset(train_, 'train', transform=transforms_train)\n    dataset_valid = CLSDataset(valid_, 'valid', transform=transforms_valid)\n    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=8, shuffle=True, num_workers=4, drop_last=True)\n    loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=8, shuffle=False, num_workers=4)\n\n    model = TimmModel('tf_efficientnetv2_s_in21ft1k', pretrained=True)\n    model = model.to(device)\n\n    optimizer = optim.AdamW(model.parameters(), lr=23e-5)\n    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n\n    metric_best = np.inf\n    loss_min = np.inf\n\n    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 75, eta_min=23e-6)\n\n    print(len(dataset_train), len(dataset_valid))\n\n    for epoch in range(1, 75+1):\n        scheduler_cosine.step(epoch-1)\n\n        print(time.ctime(), 'Epoch:', epoch)\n\n        train_loss = train_func(model, loader_train, optimizer, scaler)\n        valid_loss = valid_func(model, loader_valid)\n        metric = valid_loss\n\n        content = time.ctime() + ' ' + f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, valid loss: {valid_loss:.5f}, metric: {(metric):.6f}.'\n        print(content)\n        with open(log_file, 'a') as appender:\n            appender.write(content + '\\n')\n\n        if metric < metric_best:\n            print(f'metric_best ({metric_best:.6f} --> {metric:.6f}). Saving model ...')\n#             if not DEBUG:\n            torch.save(model.state_dict(), model_file)\n            metric_best = metric\n\n        # Save Last\n        if not DEBUG:\n            torch.save(\n                {\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scaler_state_dict': scaler.state_dict() if scaler else None,\n                    'score_best': metric_best,\n                },\n                model_file.replace('_best', '_last')\n            )\n\n    del model\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:46:33.003238Z","iopub.execute_input":"2022-10-29T08:46:33.003917Z","iopub.status.idle":"2022-10-29T08:46:33.01973Z","shell.execute_reply.started":"2022-10-29T08:46:33.003873Z","shell.execute_reply":"2022-10-29T08:46:33.018392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run(0)\nrun(1)\nrun(2)\nrun(3)\nrun(4)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:46:33.021481Z","iopub.execute_input":"2022-10-29T08:46:33.021919Z"},"trusted":true},"execution_count":null,"outputs":[]}]}