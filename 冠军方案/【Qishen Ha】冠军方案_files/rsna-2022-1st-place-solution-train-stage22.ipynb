{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1st Place Solution Training 2.5D Classification Type2\n\nHi all,\n\nI'm very exciting to writing this notebook and the summary of our solution here.\n\nThis is small version of training my final models (stage2 type2), using convnext nano as backbone, and 224x224 as input.\n\nAfter all stage1 models are trained, then we can use those model to predict 3D masks for all training samples (2k)\n\nThen use those predicted masks to crop out all vertebraes (2k * 7 = 14k)\n\nI'll skip the code of predicting 3D maks and cropping vertebraes, but just uploaded the dataset of cropped vertebraes (https://www.kaggle.com/datasets/haqishen/rsna-cropped-2d-224-0920-2m)\n\nNow let's use this dataset to train a 2.5D classification with LSTM (Type2)\n\n**NOTE: You should run it locally because it take too much GPU memory and RAM**\n\nTo see more details of my solution: https://www.kaggle.com/competitions/rsna-2022-cervical-spine-fracture-detection/discussion/362607\n\n* Train Stage1 Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage1\n* Train Stage2 (Type1) Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-train-stage2-type1\n* Train Stage2 (Type2) Notebook: This notebook\n* Inference Notebook: https://www.kaggle.com/code/haqishen/rsna-2022-1st-place-solution-inference\n\n\n**If you find these notebooks helpful please upvote. Thanks! **","metadata":{}},{"cell_type":"code","source":"DEBUG = False","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-23T06:42:07.524220Z","iopub.execute_input":"2022-11-23T06:42:07.525090Z","iopub.status.idle":"2022-11-23T06:42:07.532357Z","shell.execute_reply.started":"2022-11-23T06:42:07.525057Z","shell.execute_reply":"2022-11-23T06:42:07.531371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport gc\nimport ast\nimport cv2\nimport time\nimport timm\nimport pickle\nimport random\nimport argparse\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom PIL import Image\nfrom tqdm import tqdm\nimport albumentations\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.cuda.amp as amp\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\n%matplotlib inline\nrcParams['figure.figsize'] = 20, 8\ndevice = torch.device('cuda')\ntorch.backends.cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2022-11-23T06:42:07.534480Z","iopub.execute_input":"2022-11-23T06:42:07.535353Z","iopub.status.idle":"2022-11-23T06:42:10.506818Z","shell.execute_reply.started":"2022-11-23T06:42:07.535319Z","shell.execute_reply":"2022-11-23T06:42:10.505787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"code","source":"kernel_type = '0920_2d_lstmv22headv2_convnn_224_15_6ch_8flip_augv2_drl3_rov1p2_rov3p2_bs4_lr6e5_eta6e6_lw151_50ep'\nimage_size = 224\nn_slice_per_c = 15\ndata_dir = '../input/rsna-cropped-2d-224-0920-2m/cropped_2d_224_15_ext0_5ch_0920_2m/cropped_2d_224_15_ext0_5ch_0920_2m'\nuse_amp = True\nos.makedirs('./logs', exist_ok=True)\nos.makedirs('./models', exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-23T06:42:10.509418Z","iopub.execute_input":"2022-11-23T06:42:10.510281Z","iopub.status.idle":"2022-11-23T06:42:10.518404Z","shell.execute_reply.started":"2022-11-23T06:42:10.510243Z","shell.execute_reply":"2022-11-23T06:42:10.517406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms_train = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.HorizontalFlip(p=0.5),\n    albumentations.VerticalFlip(p=0.5),\n    albumentations.Transpose(p=0.5),  # switch X and Y axis\n    albumentations.RandomBrightness(limit=0.1, p=0.7),\n    albumentations.ShiftScaleRotate(shift_limit=0.3, scale_limit=0.3, rotate_limit=45, border_mode=4, p=0.7),\n    # Randomly apply affine transforms: translate, scale and rotate the input\n    \n    albumentations.OneOf([\n        albumentations.MotionBlur(blur_limit=3),          # Apply motion blur to the input image using a random-sized kernel\n        albumentations.MedianBlur(blur_limit=3),          # Blur the input image using a median filter with a random aperture linear size.\n        albumentations.GaussianBlur(blur_limit=3),        # Blur the input image using a Gaussian filter with a random kernel size\n        albumentations.GaussNoise(var_limit=(3.0, 9.0)),  # Apply gaussian noise to the input image\n    ], p=0.5),\n    \n    # In medical imaging problems, non-rigid transformations help to augment the data. \n    # It is unclear if they will help with this problem, but let's look at them. We will consider ElasticTransform, GridDistortion, OpticalDistortion.\n    # https://albumentations.ai/docs/examples/example_kaggle_salt/#opticaldistortion\n    albumentations.OneOf([\n        albumentations.OpticalDistortion(distort_limit=1.),\n        albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n    ], p=0.5),\n\n    albumentations.Cutout(max_h_size=int(image_size * 0.5), max_w_size=int(image_size * 0.5), num_holes=1, p=0.5),  # CoarseDropout of the square regions in the image\n])\n\ntransforms_valid = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n])","metadata":{"execution":{"iopub.status.busy":"2022-11-23T06:42:10.520004Z","iopub.execute_input":"2022-11-23T06:42:10.520726Z","iopub.status.idle":"2022-11-23T06:42:10.535886Z","shell.execute_reply.started":"2022-11-23T06:42:10.520692Z","shell.execute_reply":"2022-11-23T06:42:10.534695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataFrame","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/rsna-cropped-2d-224-0920-2m/train_seg.csv')\ndf = df.sample(16).reset_index(drop=True) if DEBUG else df\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-23T06:42:10.537631Z","iopub.execute_input":"2022-11-23T06:42:10.538287Z","iopub.status.idle":"2022-11-23T06:42:10.586708Z","shell.execute_reply.started":"2022-11-23T06:42:10.538254Z","shell.execute_reply":"2022-11-23T06:42:10.585456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class CLSDataset(Dataset):\n    # 提取出某患者7个颈椎骨的所有切片图像，每个颈椎骨能提取出15个[6, 224, 224]的图像，因此一个患者共可以提取出7*15个[6, 224, 224]的图像\n    # 训练时有一定概率将7*15个图像的顺序打乱，标签也会跟着打乱\n    def __init__(self, df, mode, transform):\n\n        self.df = df.reset_index()\n        self.mode = mode\n        self.transform = transform\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        \n        images = []\n        \n        tmp = list(range(7))\n        if self.mode == 'train' and random.random() < 0.2:\n            random.shuffle(tmp)\n        for cid in (tmp):\n            for ind in list(range(n_slice_per_c)):\n                filepath = os.path.join(data_dir, f'{row.StudyInstanceUID}_{cid+1}_{ind}.npy')\n                image = np.load(filepath)\n                # image:  [224, 224, 6]  0 - 255\n                image = self.transform(image=image)['image']\n                image = image.transpose(2, 0, 1).astype(np.float32) / 255.\n                # image:  [6, 224, 224]  0 - 1\n                images.append(image)\n        images = np.stack(images, 0)\n        # images: [7x15, 6, 224, 224]\n\n        if self.mode != 'test':\n            labels = []\n            for i in row[[f'C{x+1}' for x in tmp]].tolist():\n                labels += [i] * n_slice_per_c\n            images = torch.tensor(images).float()\n            # images: torch.Size([7x15, 6, 224, 224])\n            labels = torch.tensor(labels).float()\n            # labels: torch.Size([7x15])\n            \n            if self.mode == 'train' and random.random() < 0.2:\n                indices = torch.randperm(images.size(0))\n                images = images[indices]\n                labels = labels[indices]\n\n            return images, labels\n            # images: torch.Size([7x15, 6, 224, 224])   labels: torch.Size([7x15])\n        else:\n            return torch.tensor(images).float()","metadata":{"execution":{"iopub.status.busy":"2022-11-23T06:42:10.587896Z","iopub.execute_input":"2022-11-23T06:42:10.588330Z","iopub.status.idle":"2022-11-23T06:42:10.600955Z","shell.execute_reply.started":"2022-11-23T06:42:10.588295Z","shell.execute_reply":"2022-11-23T06:42:10.599989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rcParams['figure.figsize'] = 20,8\n\ndf_show = df\ndataset_show = CLSDataset(df_show, 'train', transform=transforms_train)\nloader_show = torch.utils.data.DataLoader(dataset_show, batch_size=8, shuffle=True, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2022-11-23T06:42:10.602113Z","iopub.execute_input":"2022-11-23T06:42:10.603064Z","iopub.status.idle":"2022-11-23T06:42:10.616492Z","shell.execute_reply.started":"2022-11-23T06:42:10.603027Z","shell.execute_reply":"2022-11-23T06:42:10.615450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rcParams['figure.figsize'] = 16,8\nf, axarr = plt.subplots(2,4)\nfor p in range(4):\n    idx = p * 20\n    imgs, lbl = dataset_show[idx]\n    axarr[0, p].imshow(imgs[35][:3].permute(1, 2, 0))\n    axarr[1, p].imshow(imgs[35][-1], cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2022-11-23T06:42:10.618098Z","iopub.execute_input":"2022-11-23T06:42:10.618994Z","iopub.status.idle":"2022-11-23T06:42:21.787801Z","shell.execute_reply.started":"2022-11-23T06:42:10.618959Z","shell.execute_reply":"2022-11-23T06:42:21.786956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class TimmModelType2(nn.Module):\n    def __init__(self, backbone, pretrained=False):\n        super(TimmModelType2, self).__init__()\n\n        self.encoder = timm.create_model(\n            backbone,\n            in_chans=6,\n            num_classes=1,\n            features_only=False,\n            drop_rate=0.,\n            drop_path_rate=0.,\n            pretrained=pretrained\n        )\n\n        if 'efficient' in backbone:\n            hdim = self.encoder.conv_head.out_channels\n            self.encoder.classifier = nn.Identity()\n        elif 'convnext' in backbone:\n            hdim = self.encoder.head.fc.in_features\n            self.encoder.head.fc = nn.Identity()\n\n        self.lstm = nn.LSTM(hdim, 256, num_layers=2, dropout=0., bidirectional=True, batch_first=True)\n        self.head = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0.3),\n            nn.LeakyReLU(0.1),\n            nn.Linear(256, 1),\n        )\n        self.lstm2 = nn.LSTM(hdim, 256, num_layers=2, dropout=0., bidirectional=True, batch_first=True)\n        self.head2 = nn.Sequential(\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.Dropout(0.3),\n            nn.LeakyReLU(0.1),\n            nn.Linear(256, 1),\n        )\n\n    def forward(self, x):  # (bs, nc*7, ch, sz, sz)\n        bs = x.shape[0]                                                  # 8\n        x = x.view(bs * n_slice_per_c * 7, 6, image_size, image_size)    # [8x7x15, 6, 224, 224]\n        feat = self.encoder(x)                                           # [8x7x15, 640]\n        feat = feat.view(bs, n_slice_per_c * 7, -1)                      # [8, 7x15, 640]\n        feat1, _ = self.lstm(feat)                                       # [8, 7x15, 512]\n        feat1 = feat1.contiguous().view(bs * n_slice_per_c * 7, 512)     # [8x7x15, 512]\n        feat2, _ = self.lstm2(feat)                                      # [8, 7x15, 512]\n \n        return self.head(feat1), self.head2(feat2[:, 0])\n        # [8x7x15, 1]   [8, 1]\n    \n\n# batch size = 8\n# ====================================================================================================\n# Layer (type:depth-idx)                             Output Shape              Param #\n# ====================================================================================================\n# TimmModelType2                                     [840, 1]                  --\n# ├─ConvNeXt: 1-1                                    [840, 640]                --\n# │    └─Sequential: 2-1                             [840, 80, 56, 56]         --\n# │    │    └─Conv2d: 3-1                            [840, 80, 56, 56]         7,760\n# │    │    └─LayerNorm2d: 3-2                       [840, 80, 56, 56]         160\n# │    └─Sequential: 2-2                             [840, 640, 7, 7]          --\n# │    │    └─ConvNeXtStage: 3-3                     [840, 80, 56, 56]         111,680\n# │    │    └─ConvNeXtStage: 3-4                     [840, 160, 28, 28]        479,680\n# │    │    └─ConvNeXtStage: 3-5                     [840, 320, 14, 14]        6,907,520\n# │    │    └─ConvNeXtStage: 3-6                     [840, 640, 7, 7]          7,448,320\n# │    └─Identity: 2-3                               [840, 640, 7, 7]          --\n# │    └─Sequential: 2-4                             --                        --\n# │    │    └─SelectAdaptivePool2d: 3-7              [840, 640, 1, 1]          --\n# │    │    └─LayerNorm2d: 3-8                       [840, 640, 1, 1]          1,280\n# │    │    └─Flatten: 3-9                           [840, 640]                --\n# │    │    └─Dropout: 3-10                          [840, 640]                --\n# │    │    └─Identity: 3-11                         [840, 640]                --\n# ├─LSTM: 1-2                                        [8, 105, 512]             3,416,064\n# ├─LSTM: 1-3                                        [8, 105, 512]             3,416,064\n# ├─Sequential: 1-4                                  [840, 1]                  --\n# │    └─Linear: 2-5                                 [840, 256]                131,328\n# │    └─BatchNorm1d: 2-6                            [840, 256]                512\n# │    └─Dropout: 2-7                                [840, 256]                --\n# │    └─LeakyReLU: 2-8                              [840, 256]                --\n# │    └─Linear: 2-9                                 [840, 1]                  257\n# ├─Sequential: 1-5                                  [8, 1]                    --\n# │    └─Linear: 2-10                                [8, 256]                  131,328\n# │    └─BatchNorm1d: 2-11                           [8, 256]                  512\n# │    └─Dropout: 2-12                               [8, 256]                  --\n# │    └─LeakyReLU: 2-13                             [8, 256]                  --\n# │    └─Linear: 2-14                                [8, 1]                    257\n# ====================================================================================================\n# Total params: 22,052,722\n# Trainable params: 22,052,722\n# Non-trainable params: 0\n# Total mult-adds (T): 2.08\n# ====================================================================================================\n# Input size (MB): 1011.55\n# Forward/backward pass size (MB): 69769.34\n# Params size (MB): 88.19\n# Estimated Total Size (MB): 70869.08\n# ====================================================================================================","metadata":{"execution":{"iopub.status.busy":"2022-11-23T06:42:21.791022Z","iopub.execute_input":"2022-11-23T06:42:21.791525Z","iopub.status.idle":"2022-11-23T06:42:21.808888Z","shell.execute_reply.started":"2022-11-23T06:42:21.791477Z","shell.execute_reply":"2022-11-23T06:42:21.807691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss & Metric","metadata":{}},{"cell_type":"code","source":"bce = nn.BCEWithLogitsLoss(reduction='none')\n\n\ndef criterion(logits, targets, activated=False):\n    if activated:\n        losses = nn.BCELoss(reduction='none')(logits.view(-1), targets.view(-1))\n    else:\n        losses = bce(logits.view(-1), targets.view(-1))\n    losses[targets.view(-1) > 0] *= 2.\n    norm = torch.ones(logits.view(-1).shape[0]).to(device)\n    norm[targets.view(-1) > 0] *= 2\n    return losses.sum() / norm.sum()","metadata":{"execution":{"iopub.status.busy":"2022-11-23T06:42:21.810472Z","iopub.execute_input":"2022-11-23T06:42:21.811111Z","iopub.status.idle":"2022-11-23T06:42:21.830061Z","shell.execute_reply.started":"2022-11-23T06:42:21.811074Z","shell.execute_reply":"2022-11-23T06:42:21.829116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train & Valid func","metadata":{}},{"cell_type":"code","source":"def mixup(input, truth, clip=[0, 1]):\n    indices = torch.randperm(input.size(0))\n    shuffled_input = input[indices]\n    shuffled_labels = truth[indices]\n\n    lam = np.random.uniform(clip[0], clip[1])\n    input = input * lam + shuffled_input * (1 - lam)\n    return input, truth, shuffled_labels, lam\n\n\ndef train_func(model, loader_train, optimizer, scaler=None):\n    model.train()\n    train_loss = []\n    train_loss1 = []\n    train_loss2 = []\n    bar = tqdm(loader_train)\n    for images, targets in bar:\n        # images: torch.Size([8, 7x15, 6, 224, 224])   targets: torch.Size([8, 7x15])\n        optimizer.zero_grad()\n        images = images.cuda()\n        targets = targets.cuda()\n        \n        do_mixup = False\n        if random.random() < 0.5:\n            do_mixup = True\n            images, targets, targets_mix, lam = mixup(images, targets)\n\n        with amp.autocast():\n            logits, logits2 = model(images)\n            # logits: [8x7x15, 1]   logits2: [8, 1]\n            loss1 = criterion(logits, targets)\n            loss2 = criterion(logits2, targets.max(1).values)\n            loss = (loss1 * 15 + loss2 * 1) / 16\n            if do_mixup:\n                loss11 = criterion(logits, targets_mix)\n                loss22 = criterion(logits2, targets_mix.max(1).values)\n                loss = loss * lam  + (loss11 * 15 + loss22 * 1) / 16 * (1 - lam)\n        train_loss1.append(loss1.item())\n        train_loss2.append(loss2.item())\n        train_loss.append(loss.item())\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        bar.set_description(f'smth:{np.mean(train_loss1[-30:]):.4f} {np.mean(train_loss2[-30:]):.4f}')\n\n    return np.mean(train_loss)\n\n\ndef valid_func(model, loader_valid):\n    model.eval()\n    valid_loss = []\n    valid_loss1 = []\n    valid_loss2 = []\n    outputs = []\n    bar = tqdm(loader_valid)\n    with torch.no_grad():\n        for images, targets in bar:\n            images = images.cuda()\n            targets = targets.cuda()\n\n            logits, logits2 = model(images)\n            loss1 = criterion(logits, targets)\n            loss2 = criterion(logits2, targets.max(1).values)\n            loss = (loss1 + loss2) / 2.\n            valid_loss1.append(loss1.item())\n            valid_loss2.append(loss2.item())\n            valid_loss.append(loss.item())\n            bar.set_description(f'smth:{np.mean(valid_loss1[-30:]):.4f} {np.mean(valid_loss2[-30:]):.4f}')\n\n    return np.mean(valid_loss)","metadata":{"execution":{"iopub.status.busy":"2022-11-23T06:42:21.831776Z","iopub.execute_input":"2022-11-23T06:42:21.832147Z","iopub.status.idle":"2022-11-23T06:42:21.849019Z","shell.execute_reply.started":"2022-11-23T06:42:21.832114Z","shell.execute_reply":"2022-11-23T06:42:21.847920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rcParams['figure.figsize'] = 20, 6\nm = TimmModelType2('convnext_nano', pretrained=True)\noptimizer = optim.AdamW(m.parameters(), lr=23e-5)\nscheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 50, eta_min=23e-6)\n\nlrs = []\nfor epoch in range(1, 50+1):\n    scheduler_cosine.step(epoch-1)\n    lrs.append(optimizer.param_groups[0][\"lr\"])\nplt.plot(range(len(lrs)), lrs)\nplt.title('Learning Rate (Epochs: 50   Max: 23e-5   Min: 23e-6)', fontsize=20)\nplt.tick_params(labelsize=15)\nplt.grid()\n_ = plt.xticks(np.arange(0, 55, 5))\n_ = plt.yticks([0, 5e-5, 10e-5, 15e-5, 20e-5, 25e-5])","metadata":{"execution":{"iopub.status.busy":"2022-11-23T06:43:12.961333Z","iopub.execute_input":"2022-11-23T06:43:12.962086Z","iopub.status.idle":"2022-11-23T06:43:13.676116Z","shell.execute_reply.started":"2022-11-23T06:43:12.962047Z","shell.execute_reply":"2022-11-23T06:43:13.675103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def run(fold):\n    log_file = os.path.join('./logs', f'{kernel_type}.txt')\n    model_file = os.path.join('./models', f'{kernel_type}_fold{fold}_best.pth')\n\n    train_ = df[df['fold'] != fold].reset_index(drop=True)\n    valid_ = df[df['fold'] == fold].reset_index(drop=True)\n    dataset_train = CLSDataset(train_, 'train', transform=transforms_train)\n    dataset_valid = CLSDataset(valid_, 'valid', transform=transforms_valid)\n    loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=8, shuffle=True, num_workers=4, drop_last=True)\n    loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=8, shuffle=False, num_workers=4)\n\n    model = TimmModelType2('convnext_nano', pretrained=True)\n    model = model.to(device)\n\n    optimizer = optim.AdamW(model.parameters(), lr=23e-5)\n    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n    from_epoch = 0\n    metric_best = np.inf\n    loss_min = np.inf\n\n    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 50, eta_min=23e-6)\n\n    print(len(dataset_train), len(dataset_valid))\n\n    for epoch in range(1, 50+1):\n        scheduler_cosine.step(epoch-1)\n        print(time.ctime(), 'Epoch:', epoch)\n\n        train_loss = train_func(model, loader_train, optimizer, scaler)\n        valid_loss = valid_func(model, loader_valid)\n        metric = valid_loss\n\n        content = time.ctime() + ' ' + f'Fold {fold}, Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {train_loss:.5f}, valid loss: {valid_loss:.5f}, metric: {(metric):.6f}.'\n        print(content)\n        with open(log_file, 'a') as appender:\n            appender.write(content + '\\n')\n\n        if metric < metric_best:\n            print(f'metric_best ({metric_best:.6f} --> {metric:.6f}). Saving model ...')\n            torch.save(model.state_dict(), model_file)\n            metric_best = metric\n\n        # Save Last\n        if not DEBUG:\n            torch.save(\n                {\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scaler_state_dict': scaler.state_dict() if scaler else None,\n                    'score_best': metric_best,\n                },\n                model_file.replace('_best', '_last')\n            )\n\n    del model\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:12:38.503102Z","iopub.execute_input":"2022-10-29T08:12:38.503712Z","iopub.status.idle":"2022-10-29T08:12:38.522409Z","shell.execute_reply.started":"2022-10-29T08:12:38.503668Z","shell.execute_reply":"2022-10-29T08:12:38.521354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run(0)\nrun(1)\nrun(2)\nrun(3)\nrun(4)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T08:12:42.821292Z","iopub.execute_input":"2022-10-29T08:12:42.821672Z"},"trusted":true},"execution_count":null,"outputs":[]}]}